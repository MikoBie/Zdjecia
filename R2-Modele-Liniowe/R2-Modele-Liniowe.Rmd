---
title: "R2 | Modele Liniowe"
author: "Mikołaj Biesaga, Szymon Talaga, ISS UW"
date: '`r Sys.Date()`'
output:
  html_notebook:
    toc: true
  html_document:
    toc: true
---

<hr>

```{r basic_packages}
suppressPackageStartupMessages({
    library(magrittr)
    library(dplyr)
    library(feather) #trzeba najpierw ten pakiet zainstalować
    library(car)
    library(tidyr)
})
```

## Wprowadzenie

Ogólny model liniowy jest prawdopodobnie najważniejszym modelem zależności między zmiennymi w całej stosowanej statystyce. Jego proste uogólnienia pozwalają na analizę bardzo wielu różnych scenariuszy i układów zmiennych (w tym również zmiennych porządkowych i jakościowych). Jest on również podstawą, na której zbudowana jest znaczna część najważniejszych metod nieliniowych. Dlatego poświęcimy mu tu dość dużo miejsca.

### Podstawy modeli liniowych w `R`

W pierwszej części skupimy się na praktycznym wprowadzeniu do modeli liniowych w `R` tak, aby każdy mógł się najpierw zaznajomić na podstawowym poziomie ze specyfiką ich wykorzystywania w środowisku obliczeniowym języka `R`. Najpierw jednak przypomnimy podstawową strukturę każdego modelu liniowego. Najbardziej podstawowym modelem liniowym jest prosta jednozmiennowa regresja liniowa, która opisana jest równaniem:

$$y_i = b_0 + b_1x_i + e_i$$
gdzie:

 * $y_i$ to wartość zmiennej zależnaj dla $i$-tej obserwacji
 * $x_i$ to wartość zmiennej niezależnej dla $i$-tej obserwacji
 * $b_0$ to stała równania regresji
 * $b_1$ to współczynnik kierunkowy zmiennej niezależnej
 * $e_i$ to reszta równania regresji dla $i$-tej obserwacji

Powyższe to zatem po prostu równanie funkcji liniowej jednej zmiennej z dodanym błędem określającym odchylenie linii regresji od rzeczywistej wartości zmiennej zależnej.

Model jednozmiennowy w naturalny sposób uogólnia się do modelu wielozmiennowego, czyli:

$$y_i = b_0 + b_1x^{(1)}_i + \ldots + b_mx^{(m)}_i + e_i$$

Tyle słowem wstępu. Dokładne założenia oraz mechanikę modeli liniowych omówimy w kolejnej części. Teraz skupimy się na praktycznym wykorzystaniu tego modelu w `R`. W tym podstawowym wprowadzeniu dalej będziemy używać dobrze nam znanego zbioru danych `iris` (choć w dalszej części zaczniemy sięgać po inne, bardziej złożone zbiory danych, które pozwolą nam zilustrować niektóre ważne właściwości różnych modeli).

#### Konstrukcja i interpretacja prostych modeli

Modele liniowe w `R` (a tak naprawdę większość modeli statystycznych) jest związana z tzw. interefejsem formułowym. Jest to niezwykle wygodne narzędzie, które pozwala na bardzo czytelne definiowanie obiektów modeli.

```{r lm_univariate_formula}
### Model liniowej zależności między długością płatka (Sepal.Length)
### jako zmienną zależną a długością kielicha (Petal.Length)
model1 <- lm(Sepal.Length ~ Petal.Length, data = iris)

### Co się powyżej wydarzyło? Rozłóżmy powyższe wywołanie na części pierwsze
###
### 1. Po prawej stronie operator przypisania (`<-`) mamy wywołanie funkcji `lm`.
### 2. `lm` to funkcja tworząca obiekt modelu liniowego (warto zerknąć do dokumentacji, `?lm`)
### 3. Zawsze pierwszym argumentem funkcji `lm` jest tzw. formuła;
###    tutaj jest to wyrażenie `Sepal.Length ~ Petal.Length`.
### 4. Po prawej strony operatora `~` mamy zmienną zależną (tak jak w równaniu regresji)
### 5. Po lewej stronie mamy zmienną niezależną (też dokładnie tak jak w równaniu regresji)
### 6. Ostatnim (bardzo ważnym elementem) wywołania funkcji `lm` jest argument `data`.
###    Określa on w jakim zbiorze danych `R` ma szukać nazw, których użyto w formule
###    (zauważmy, że przekazujemy te nazwy jako nazwy obiektów, więc R musi wiedzieć,
###     gdzie szukać przypisanych im wartości).

### Teraz zobaczmy, co mówi nam taki model liniowy, jak każemy wypisać go w konsoli
model1
```

Efekt wypisania modelu już coś nam mówi, ale tak naprawdę niewiele. Pokazuje jedynie jak wywołano funkcję, która stowrzyła ten obiekt oraz wartości oszacowanych współczynników (`(Intercept)` to oczywiście stała równania regresji). Chcielibyśmy jednak uzyskać więcej informacji, np. na temat istotności współczynników, czy siły efektu modelu $R^2$. Na szczęście jest to bardzo proste. Wszystkie tego typu informacje pozwala uzyskać funkcja `summary`.

```{r lm_univariate_sumamry}
### Podsumowanie modelu
summary(model1)

### Jak widać pokazuje nam ona szczegółowe podsumowanie modelu
### wraz z wynikami testów istotności poszczególnych współczynników
### oraz ogólnym testem F (który jak wiadomo zakłada hipotezę zerową R^2 = 0),
### czyli inaczej mówiąc analizą wariancji modelu.
###
### Mamy również podane R^2 oraz szereg innych statystyk (np. skorygowane R^2)
### czy błąd standardowy reszt.
```

```{r lm_univariate_confint}
confint(model1)
```

Domyślnie zwraca ona przedziały 95-procentowe, ale to można zmienić za pomocą argumentu `level` (por. `?confint`).

W tym momencie przypomnijmy po krótce interpretację współczynników w modelu liniowym. Wartość współczynnika `(Intercept)` to tzw. stała równania, czyli przewidywana wartość zmiennej zależnej w sytuacji, gdy wszystkie zmienne niezależne przyjmują wartość 0. Współczynnik zmiennych niezależnych (tu jest to `Petal.Length`) określają o ile (wedle modelu) wyższy jest przeciętnie poziom zmiennej zależnej obserwacji, u których wartość zmiennej niezależnej jest wyższa o jedną jednostkę. W przypadku regresji wielokrotnej (z więcej niż jedną zmienną zależną) współczynnik ten określa przeciętną różnicę między obserwacjami, które **są takie same** poza tym, że jedna z nich ma wartość danej zmiennej niezależnej wyższą o jedną jednostkę.

To są oczywiście podstawy, które prawie każdy zna, ale warto je przypomnieć, ponieważ dobre, intuicyjne zrozumienie równania regresji okazuje się być niezwykle ważne przy interpretacji bardziej skomplikowanych modeli.

`R` pozwala również na bardzo łatwą (oraz miłą estetycznie) wizualizację modeli liniowych. Wykorzystamy w tym celu niezwykle popularny pakiet graficzny `ggplot2`.

```{r lm_univariate_viz}
### Wczytanie pakietu i ustawienie szaty kolorystycznej.
### Domyślna jest niestety dość brzydka (naszym zdaniem).
library(ggplot2); theme_set(theme_bw())

# Wywołanie funkcji rysującej wykres
# Specjalnie rozpisaliśmy je na wiele linii, aby móc dokładnie opisać każdy wiersz
#
iris %>%    # za pomocą operatora potokowego przekazujemy ramkę danych `iris` do funkcji rysującej
    ggplot(     # wywołujemy funkcję rysującą
        aes(    # definujemy tzw. obiekt estetyki; definiuje on logikę wykresu
            x = Petal.Length,   # w nim określamy zmienną odkładaną na osi X (zmienna niezależna)
            y = Sepal.Length    # oraz zmienną odkładaną na osi Y (zmienna zależna)
        )
    ) +   # operatorem "+" (który ma tu specjalne znaczenie), dodajemy kolejne elementy wykresu
    geom_point() +    # mówimy żeby wykres narysował dane jako punkty
    geom_smooth(method = "lm")    # oraz dodał do nich linię trendu prostej regresji liniowej

# W rezultacie otrzymujemy schludny wykres, na którym dodatkowo naniesiony jest graficznie
# przedział ufności dla oszacowanej linii regresji.
```

Zbudowanie modelu liniowego dla więcej niż jednej zmiennej jest równie proste. Chwilowo uznamy model z kilkoma ilościowymi zmiennymi niezależnymi za dość trywialny (zasadnicza interpretacja współczynników pozostaje podobna jak w modelu z jedną zmienną, choć nie identyczna) i skupimy się na modelu z dodaną zmienną jakościową. Zwłaszcza, że jak się za chwilę okaże, zbiór `iris` jest wyjątkowo wdzięcznym materiałem do takiej analizy.

Najpierw jednak pochylmy się przez chwilę nad najprostszym modelem liniowym z jedną zmienną jakościową. W tym celu przeprowadzimy znany wszystkim test $t$-studenta przy użyciu menażerii modeli liniowych, jaką dostarcza nam *R* (choć oczywiście `R` posiada również dedykowaną temu problemowi funkcję `t.test`).

```{r lm_univariate_binary}
### Wygenerujemy proste dane losowe
df <- data.frame(
    y = c(
        rnorm(100, 100, 15),   # 100 realizacji zmiennej losowej o rozkładzie N(100, 15)
        rnorm(100, 90, 12)     # 100 realizacji zmiennej losowej o rozkładzie N(90, 12)
    ),
    x = rep(c("A", "B"), each = 100)    # zmienna grupująca (100 razy A i potem 100 razy B)
)

### Model linowy badający różnice między grupami A i B
modeltt <- lm(y ~ x, data = df)
summary(modeltt)

### Standardowy test t
(tt <- t.test(y ~ x, data = df, var.equal = TRUE))

### Jak widać różnica między średnimi grupowymi z testu t
### a oszacowanym współczynnikiem regresji wynosi 0
### tj. obie wartości są sobie równe
all.equal(coef(modeltt)[2], (tt$estimate[2] - tt$estimate[1]), check.attributes = FALSE)
```

Analizując uważnie wyniki podane przez obie funkcje łatwo również zauważyć, że sama konstrukcja testu $t$ jest w obu przypadkach identyczna. I w jednym i drugim przypadku statystyka testowa $t$ wynosi 5.213 i ma 198 stopni swobody. Poziomy istotności również są takie same. Zatem obie metody są w tym przypadku równoważne. I nie jest to przypadek, test $t$ w swojej podstawowej wersji jest tak naprawdę szczególnym przypadkiem modelu liniowego (choć w praktyce zazwyczaj stosuję się tzw. korektę Welscha na nierówne wariancje w grupach i w takim przypadku już nie mamy takiej idealnej równoważności względem modelu liniowego).

Pokazaliśmy ten trywialny przykład głównie dlatego, żeby mieć pretekst do pojawiającego się w takim kontekście równania regresji. Jest ono postaci:

$$y_i = b_0 + b_1x_i + e_i$$

Czyli jest dokładnie takie jakie być powinno. Niemniej jednak zmienna niezależna ma tu szczególną postać, bo jest zmienną zerojedynkową, która wskazuje przynależność do pewnej grupy. W ten sposób jej współczynnik regresji staje się po prostu miarą różnicy między średnią w tej grupie a średnią w grupie odniesienia (czyli w tym przypadku drugiej grupie, w której to zmienna zależna $x$ przyjmuje wartość zero).

To są oczywiście podstawy, które każdy zna. Przypomnieliśmy je jednak w ramach rozgrzewki, bo myślenie w kategoriach równania regresji będzie później bardzo ważne przy interpretacji bardziej skomplikowanych modeli.

Teraz przechodzimy do odrobinę bardziej skomplikowanego przypadku, czyli przypadku interakcji między zmienną ilościową i jakościową. Czyli teraz wracamy do zbioru z powrotem do zbioru `iris`.

Poniżej estymujemy i podsumowujemy model przewidujący długość płatka (`Sepal.Length`) na podstawie długości kielicha (`Petal.Length`) w interakcji z gatunkiem (`Species`).

```{r lm_bivariate_quantqual}
model2 <- lm(Sepal.Length ~ Petal.Length * Species, data = iris)
summary(model2)
```

Jaką interpretacje mają otrzymane tu współczynniki? Regresory zerojedynkowe (`dummy variables/regressors`) dla zmiennej `Species` wskazują gatunki `versicolor` albo `virginica`, co oznacza, że w naszym modelu grupą odniesienia jest gatunek `setosa`. Zatem stała modelu to przewidywany poziom zmiennej zależnej dla gatunku `setosa` przy zerowym `Petal.Length`. Jednocześnie współczynnik dla regresora `Petal.Length` to tak naprawdę współczynnik kierunkowy **jedynie** dla gatunku `setosa`. W przypadku pozostałych gatunków należy dodatkowo uwzględnić regresory modelujące interakcję między danym gatunkiem a zmienną `Petal.Length`. W ten sposób model przyzwala na współistnienie różnych współczynników kierunkowych w różnych grupach.

Zatem ostatecznie z modelu wyłaniają sie trzy równania regresji, po jednym dla każdego z gatunków:

* **setosa:** $y_i = 4.21 + 0.54 \times Petal.Length$
* **versicolor:** $y_i = (4.21 - 1.81) + (0.54 + 0.29) \times Petal.Length$
* **virginica:** $y_i = (4.21 - 3.15) + (0.54 + 0.45) \times Petal.Length$

Poniżej przedstawiamy wizualizację otrzymanego modelu.

```{r lm_bivariate_quantqual_viz}
### Ponownie użyjemy pakietu ggplot2.
### Jak zaraz zobaczymy, wystarczy dodać jeden argument,
### aby uzyskać łądny wykres, w którym grupujemy dane wedle gatunków.
iris %>%    # za pomocą operatora potokowego przekazujemy ramkę danych `iris` do funkcji rysującej
    ggplot(     # wywołujemy funkcję rysującą
        aes(    # definujemy tzw. obiekt estetyki; definiuje on logikę wykresu
            x = Petal.Length,   # w nim określamy zmienną odkładaną na osi X (zmienna niezależna)
            y = Sepal.Length,   # oraz zmienną odkładaną na osi Y (zmienna zależna)
            #### NOWA LINIA ###
            color = Species     # grupujemy wedle zmiennej `Species` i oznaczamy grupy różnymi kolorami
            ###################
        )
    ) +   # operatorem "+" (który ma tu specjalne znaczenie), dodajemy kolejne elementy wykresu
    geom_point() +    # mówimy żeby wykres narysował dane jako punkty
    geom_smooth(method = "lm")    # oraz dodał do nich linię trendu prostej regresji liniowej
```

Na zakończenie tej sekcji pochylimy się nad najważniejszym i jednocześnie najbardziej klasycznym rodzajem modeli liniowych czyli 2-czynnikowej analizie wariancji. Wykorzystamy do tego pewien rzeczywisty ale całkowicie zaciemniony zbiór danych, który wczytamy z pliku binarnego. W zbiorze tym będziemy mieli dwa 2-poziomowe czynniki jakościowe oraz ilościową zmienną zależną. Dodatkowo liczebności poszczególnych grup nie będą w pełni zrównoważone, co pozwoli na zobraozwanie najważniejszych różnic między różnymi typami sum kwadratów, które są bardzo ważne w analizie wariancji.

```{r twoway_load_data}
df <- read_feather("twoway.feather")

# Układ czynników jest niezrównoważony
table(df$A, df$B)
```

```{r twoway_unbalanced_model}
twmod1 <- lm(X ~ A * B, data = df)
twmod2 <- lm(X ~ B * A, data = df)

# Analiza wariancji typu I (sekwencyjna) dla modelu 1
anova(twmod1)
# Analiza wariancji typu I (sekwencyjna) dla modelu 2
anova(twmod2)
```

Zauważmy, że choć w obu przypadkach otrzymaliśmy podobne wyniki, to jednak nie są one identyczne. Różnią się zarówno na poziomie wartości sum kwadratów jak i samych statystyk testowych i co za tym idzie obliczonych poziomów istotności efektów. Czemu tak jest? Otóż jest tak dlatego, że `R` w funkcji `anova` używa **tylko i wyłącznie** tzw. sum kwadratów typu I czyli inaczej sekwencyjnych sum kwadratów. Dla użytkowników pakietów takich jak *SPSS* może to być zaskakujące, gdyż tam domyślnie wykorzystuje się sumy typu III, które działają inaczej. Jednak twórcy `R` mieli bardzo dobry powód, żeby uwzględniać tylko sumy typu pierwszego: pozostałe typu sum kwadratów (II i III i inne) tak naprawdę **nie istnieją**. Mówimy to tu w tym sensie, że inne typy sum kwadratów są tak naprawdę tylko szczególnymi przypadkami sum typu I i zawsze można zdefiniować taki model liniowy, w którym sumy kwadratów typu I będą równe pewnej sumie kwadratów innego typu.

W jaki dokładnie sposób działa suma kwadratów typu I? Analiza wariancji w tym modelu polega na testowaniu czynników dodawanych pojedynczo w kolejności w jakiej zostały uwzględione w modelu. Zatem w modelu ze specyfikacją `X ~ A*B` najpierw testowany jest efekt `A|1` (sprawdza, czy średnie grupowe dla czynnika A są różne od siebie; w tym przypadku jest to w przybliżeniu ekwiwalent testu $t$), następnie efekt `B|A` (istotność czynnika B po uwzględnieniu efektu `A`) i na koniec istotność czynnika interakcji `A:B|A, B`. Właśnie dlatego kolejność czynników w równaniu modelu ma tu znaczenie.

Sumy II i III typu są pozornie bezużyteczne. Tak naprawdę są bardzo użyteczne, gdyż oszczędzają mnóstwo roboty, którą trzeba by spędzić na definiowaniu dodatkowych modeli testujących interesujące nas hipotezy, i w praktyce używa się ich bardzo często. Dlatego społeczność zorganizowana wokół `R` szybko przygotowała implementacje analizy wariancji z pozostałymi najważniejszymi typami sum kwadratów czyli typem II i III. Można je znaleźć w funkcji `Anova` z popularnego pakietu `car`.

```{r twoway_unbalanced_model_anova}
# Anova z sumami typu II
Anova(twmod1, type = 2)

# Anova z sumami typu III
Anova(twmod1, type = 3)
```

Czemu wyniki w obu przypadkach są trochę inne? Ponieważ obie sumy kwadratów zorientowane są na testowanie trochę innych efektów, i jak się zaraz okaże typ III jest w większości przypadków **pozbawiony sensu**, pomimo że jest to najczęściej stosowany typ sum kwadratów (ale jest to błąd!!!).

Sumy kwadratów typu II stosoują się tzw. zasady brzegowości (*principle of marginality*), czyli nie testują nigdy bezsensownych hipotez, w których z wariancji efektów nadrzędnych wyłącza się wariancję wyjaśnianą przez efekty w nich zagnieżdżone. Bardziej konkretnie, w naszym przypadku Anova z sumami typu II testuje następujące efekty:

 * `A | B` (efekt `A` z wyłączeniem wariancji wyjaśnianej przez `B`)
 * `B | A` (efekt `B` z wyłączeniem wariancji wyjaśnienej przez `A`)
 * `A:B | A, B` (efekt interakcji `A` i `B` po wyłączeniu wariancji wyjaśnianej przez oba efekty główne)

Siłą sum kwadratów typu II jest właśnie to, że każda z powyższych hipotez jest praktycznie zawsze **interpretowalna**. Jednocześnie tego samego nie można powiedzieć o sumach kwadratów typu III, które testują następujące efekty:

 * `A | B, A:B` (efekt `A` z wyłączeniem wariancji wyjaśnianej przez `B` oraz interakcję `A` i `B`)
 * `B | A, A:B` (efekt `B` z wyłączeniem wariancji wyjaśnianej przez `A` oraz interakcję `A` i `B`)
 * `A:B | A, B` (efekt interakcji `A` i `B` po wyłączeniu wariancji wyjaśnianej przez oba efekty główne)

Jak łatwo zauważyć efekt interakcji jest w obu przypadkach testowany w taki sam sposób (co widać również w tabelach analizy wariancji, które otrzymaliśmy). Jednocześnie sposób testowania efektów głównych jest w przypadku sum typu III niezwykle problematyczny, bo testowane są efekty, które w większości przypadków nie mogą mieć żadnej sensownej interpretacji. W rezultacie testy efektów głównych w analizie wariancji z sumami typu 3-ciego mają prawie zawsze mniejszą moc (bo niepotrzebnie wyłączamy wariancje powiązaną z efektem interakcji).

#### Modele multiplikatywne

Jakkolwiek standardowy model liniowy jest z definicji addytywny, to może być wykorzystany do modelowania bardzo wielu różnych zjawisk i procesów nieliniowych, w tym niektórych procesów multiplikatywnych. Kanonicznym przykładem takiego zastosowania jest model liniowy, w którym zmienną zależną poddano przekształceniu logarytmicznemu.

Załóżmy, że mamy model postaci:

$$log(y) = b_0 + b_1x$$
Widać, że dopóki pozostawiamy w zmienną zależną w skali logarytmicznej, to nasz model jest liniowy i addytywny. Zobaczmy jednak, co się stanie, gdy przedstawimy model na skali właściwej dla nieprzekształconej zmiennej $y$.

 1. Dokonujemy eksponencjacji obu stron powyższego równania: $y = e^{b_0 + b_1x}$
 2. Co wykorzystując arytmetykę potęgowania można zapisać: $y = e^{b_0}e^{b_1x}$
 3. $e^{b_0}$ to stała, więc chwilowo można ją pominąć: $y \propto e^{b_1x}$
 4. Teraz widać, że zwiększając $x$ o 1 mamy: $e^{b_1(x+1)} = e^{b_1}e^{b_1x}$

Zatem w takim modelu każdorazowy wzrost zmiennej niezależnej o jedną jednostkę powoduje wzrost poziomu zmiennej zależnej o czynnik $e^{b_1}$ (innymi słowy wartości sprzed wzrostu zostaje przemnożona przez stałą równą $e^{b_1}$). W ten sposób regresja liniowa (z natury addytywna) może być wykorzystana do modelowania niektórych zjawisk o multiplikatywnym charakterze.

Wykorzystywanie transformacji logarytmicznej jest niezwykle użyteczne. Przede wszystkim pozwala badać niektóre zjawiska o nieaddytwnym charakterze, a także (co jest powiązane) w wielu przypadkach jest dobrym narzędziem stabilizowania niehomogenicznej wariancji reszt.

```{r log}
# Tworzymy sztucznie zmiennę o rozkładzie lognormalnym
# oraz skorelowaną z ną zmienną o rozkładzie normalnym
set.seed(101010)
y <- rlnorm(1000, meanlog = 8, sdlog = 1)
x <- log(y) + rnorm(1000, 0, 1)
```

```{r log_model1}
lmod1 <- lm(y ~ x)
lmod2 <- lm(log(y) ~ x)

# Podsumowanie dla modelu bez transformacji logarytmicznej
summary(lmod1)

# Podsumowanie dla modelu z tranformcją logarytmiczną
summary(lmod2)
```

```{r log_resid}
df <- data.frame(
    Model = rep(c("Normalny", "Log"), each = 1000),
    Wartość = c(resid(lmod1), resid(lmod2))
)
df %>%
    ggplot(aes(x = Wartość)) +
    geom_histogram(color = "black") +
    facet_wrap(~Model, scales = "free") +
    ylab("")
```

Powyższe wykresy pokazują jak bardzo dobrze sprawdziła się w tym przypadku transformacja logarytmiczna. Nalezy jednak zawsze pamiętać o tym, że zastosowanie tej transformacji będzie miało **zawsze** daleko idące skutki konsekwencje względem interpretacji modelu, bo zmienia model addytywny w model multiplikatywny. Z tego same powodu transformacja logarytmiczna jest również więcej niż tylko pewną sztuczką służącą do normalizowania rozkładów i stabilizowania wariacji.

#### Przekształcenie logitowe

Drugim użytecznym i ciekawym przekształceniem jest tzw. przekształcenie logitowe. Przyjrzymy mu się uważnie też przy okazji omawiania regresji logistycznej, jednak warto je też omówić w kontekście zwykłej regresji liniowej. Tak naprawdę w wielu przypadkach najważniejszym, a prawie nigdy nieartykułowanym, założeniem regresji liniowej jest to, że zmienna zależna może być modelowana jako *zmienna rzeczywista* ($y \in \mathbb{R}$). Jest to ważne, ponieważ regresja liniowa, jako prosty model addytywny zakłada liniowy wzrost/spadek zmiennej zależnej bez żadnych ograniczeń (tj. tak długo, jak długo zmieniają się wartości zmiennych predyktorów). To pokazuje, że np. modelowanie wartości, które są zliczaniemi, a więc $y \in \mathbb{N}$, w prostym modelu liniowym czasami może nie być dobrym pomysłem (dlatego powstały takie narzędzia jak np. regresja poisssona). Podobnie jest w przypadku modelowania odsetków/prawdopodobieństw. Gdy mamy do czynienia ze zdarzeniami binarnymi (0/1), wówczas należy sięgnąć po regresję logistyczną (którą omówimy za jakiś czas). Jednak w niektórych przypadkach naszą zmienną zależną jest właściwe prawdopodobieństwo, czy też odsetek. W takim przypadku mamy do czynienia po prostu ze zmienną zależną $y \in (0, 1)$. Widać wyraźnie, że nasza zmienna jest zdefiniowana na jedynie maleńkim podzbiorze linii liczb rzeczywistych, więc regresja liniowa może bardzo łatwo dać to bezsesnowne rezultaty. Pierwszym krokiem przed właściwą analizą powinno więc być takie przekształcenie zmiennej zależnej, aby rozkładała się ona po całym zbiorze liczb rzeczywistych. Do tego właśnie służy transformacja logitowa. Ma ona następującą postać:

$$logit(y) = log(\frac{y}{1-y})$$

Gdzie:
 * $y \in (0, 1)$

Czemu powyższa transformacja działa? Łatwo to dostrzec rozbijając wzór na elementy składowe. Dla uproszczenia będziemy interpretować $y$ jako miarę prawdopodobieństwa (a więc odsetki też się kwalifikują). Zauważmy, że pierwsze przekształcenie to po prostu $\frac{y}{1-y}$. Jest to wzór na tzw. szanse (*odds*), czyli stosunek prawdopodobieństwa sukcesu do prawdopodobieństwa porażki. Nasze $y$ wedle przyjętego założenia nigdy jest dodatnie, więc mamy $\frac{y}{1-y} \in (0, \infty)$. Następnie mamy $\log(\frac{y}{1-y}) \in \mathbb{R}$. W ten właśnie sposób otrzymujemy ostatecznie zmienną rzeczywistą, którą możemy swobodnie modelować przy użyciu regresji liniowej.

Wykorzystamy tę okazję, aby wprowadzić bardzo ważny i niezwykle przydatny koncept programistyczny czyli definiowanie funkcji. Funkcja w programowaniu jest analogiem funkcji w matematyce: to magiczne pudełko, które przyjmuje jakieś dane wejściowe i w zamian zwraca jakieś dane wyjściowe. Każda funkcja ma również swoją domenę czyli zbiór wartości argumentów (danych wejściowych), które akceptuje (dla których jest zdefiniowana). Zobaczmy, jak wygląda w R funkcja licząca logit.

```{r logit_func}
logit <- function(x) {
    # Ciało funkcji; 
    # tu definiujemy operacje, które przekształcają dane wejściowe w dane wyjściowe
    # Sprawdzamy, czy argument wejściowy jest poprawny
    stopifnot(x >= 0, x <= 1)
    # Obliczamy wartość logitu i zwracamy ją
    log(x / (1 - x))
}

# Następnie wywołujemy naszą funkcję tak jak każdą inną funkcję w R
logit(.8)
```

Wykorzystajmy ją teraz do przeprowadzenia prostej analizy.

```{r logit_naive_model}
# Przygotowanie sztucznych danych
set.seed(101)
y <- runif(1000)    # odsetki pobrane z rozkładu jednostajnego
x <- y + rnorm(1000, sd = 1/2)  # zmienna o rozkładzie normalnym skorelowana z x

# Naiwny model (bez transformacji zmiennej zależnej)
lmod0 <- lm(y ~ x)
summary(lmod0)

# Pomocnicza ramka danych do wizualizacji przy użyciu pakietu `ggplot2`
df <- data.frame(x = x, y = y)

# Wykres rozrzutu dla zmiennych z nałożoną nieparametryczną krzywą najlepszego dopasowania
df %>%
    ggplot(aes(x = x, y = y)) +
    geom_point(alpha = .3) +
    geom_smooth(method = "loess")
```

```{r logit_model}
# Wykres rozrzutu z przkeształconą zmienną zależną
df %>%
    ggplot(aes(x = x, y = logit(y))) +
    geom_point(alpha = .3) +
    geom_smooth(method = "loess")

# Model z przekształconą zmienną zależną
lmod1 <- lm(logit(y) ~ x)
summary(lmod1)
```

Pierwszy skonstruowany model był postaci:

$$y = 0.36 + 0.26x$$
Widać, że przy odpowiednio dużym/małym $x$ może on generować bezsensowne wartości $y$ będące poza zbiorem $(0, 1)$. Drugi model daje za to następujące równanie:

$$log(\frac{y}{1-y}) = -0.83 + 1.59x$$
W tym momencie warto dokonać kilku przekształceń algebraicznych, żeby sprowadzić model z poziomu trudnych w interpretacji logarytmów szans (*log-odds*) do poziomu zrozumiałych dla człowieka prawdopodobieństwa. Najpierw schodzimy do poziomu szans poprzez eksponencjaję równania stronami:

$$\frac{y}{1-y} = e^{-0.83 + 1.59x}$$
Stąd już łatwo przejść do wyrażenia określającego prawdopodobieństwo:

$$y = \frac{e^{-0.83 + 1.59x}}{1 + e^{-0.83 + 1.59x}}$$

Widzimy więc, że istotnie otrzymaliśmy funkcję określającą $y$, która nie może przyjąć wartości spoza zbioru $(0, 1)$ niezależnie od poziomu zmiennej niezależnej $x$. Otrzymaliśmy w efekcie funkcję nieliniową.

Zobaczmy, jak zachowują się oba modele, gdy wykorzystamy je do predykcji na przedziale $x \in [-2.5, 2.5]$.

```{r logit_naive_model_pred}
# Zdefiniujmy funkcję odwrotną do funkcji logitowej
invlogit <- function(x) {
    exp(x) / (1 + exp(x))
}

# Tworzymy ramkę danych z siatką wartości predyktorów (tu jednozmiennową)
df <- data.frame(x = seq(from = -2.5, to = 2.5, by = 0.01))
# Wykorzystjemu oba modele do dokonania predykcji w oparciu o te dane
df$y <- predict(lmod0, newdata = df)
df$logit_y <- invlogit(predict(lmod1, newdata = df))

# Zobaczmy naszą ramkę danych
head(df)
# Wykorzystajmy metodą `gather` z pakietu `tidyr`
# aby z dwóch kolumn stworzyć jedną, co ułatwi jednoczesną wizualizację obu modeli
df <- gather(df, key = "wskaźnik", value = "y", -x)

# Zobaczmy ramkę danych raz jeszcze
head(df)

# Zwizualizujmy predykcję obu modeli
df %>%
    ggplot(aes(x = x, y = y, col = wskaźnik, linetype = wskaźnik)) +
    geom_line() +
    geom_hline(yintercept = c(0, 1), linetype = 2)
```

Widzimy, że transformacje logitowa grzecznie zachowuje narzucone jej warunki co do zakresu przyjmowanych wartości, czego nie można powiedzieć o prostej regresji liniowej.

<hr>

<!-- CSS styling -->
<style>
    html {
        height: 100%;
        font-size: 62.5%;
    }
    body {
        height: 100%;
        font-size: 1.6em;
        font-family: "Trebuchet MS", "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", sans-serif;
    }
    h1, h2, h3 {
        text-align: center;
    }
    h4.author, h4.date {
        margin: 0.75em 0 0 0;
        text-align: center;
    }
    h2, h3, h4, h5, h6 {
        margin: 2em 0 1em 0;
    }
    div#header {
        margin: 1em 0 1em 0;
    }
    hr {
        margin: 2em 0 2em 0;
    }
    pre {
        margin-bottom: 2em;
    }
</style>

<!-- End of styling -->